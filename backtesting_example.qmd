---
title: "Backtesting with ML models"
format:
    html:
        code-fold: false
jupyter: python3
---

Let's train regression models with different regression models.

## Loading the data

Utilies and data libraries
```{python}
import pandas as pd
import datetime
import json
```

Load the metadata
```{python}
#| label: Load_metadata
raw_json = open("./settings/settings.json")
meta_json = json.load(raw_json)
backtesting_path = meta_json["data"]["backtesting_path"]
```

### Load the dataset and reformat it
```{python}
# | label: Load_the_data

ts = pd.read_csv("data/data.csv")
ts["ds"] = pd.to_datetime(ts["ds"])
ts = ts.sort_values("ds")
ts = ts[["unique_id", "ds", "y"]]
ts.head()
```

Set the data
```{python}
# | label: Subset_the_data
# os.environ["NIXTLA_ID_AS_COL"] = "1"
```


```{python}
# | label: Plot_the_series
from utilsforecast.plotting import plot_series
plot_series(ts, engine = "plotly").update_layout(height=300)
```
## Set the Backtesting Process

Let's start by defining the models:

- Regression based on k-nearest neighbors (see model [documentation](https://scikit-learn.org/1.5/modules/generated/sklearn.neighbors.KNeighborsRegressor.html))
- Multi-layer Perceptron regressor (see model [documentation](https://scikit-learn.org/1.5/modules/generated/sklearn.neural_network.MLPRegressor.html))
- ElasticNet - Linear regression with combined L1 and L2 priors as regularizer (see model [documentation](https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.ElasticNet.html))


```{python}
# | label: set_models
from sklearn.linear_model import  ElasticNet
from sklearn.neighbors import KNeighborsRegressor
from sklearn.neural_network import MLPRegressor

ml_models = {
    "knn": KNeighborsRegressor(),
    "mlp": MLPRegressor(),
    "enet": ElasticNet()
}
```

And than define the forecast object:

```{python}
# | label: set_forecast_obj
from mlforecast import MLForecast
from mlforecast.utils import PredictionIntervals
mlf = MLForecast(
    models=ml_models,
    freq="h",
    lags=list(range(1, 24)),
    date_features=["month", "day", "dayofweek", "week", "hour"]
)
```

Let's now define the backtesting parameters. We will use a backtesting with four testing partitions, each testing partition with length of 72 hours, and overlapping of 12 hours between each partition. In adddion we will set a 95% prediction intervals using conformal distribution method:

```{python}
#| label: backtesting_settings
h = 72
step_size = 12
partitions = 4
n_windows = 3
method = "conformal_distribution"
pi = PredictionIntervals(h=h, n_windows = n_windows, method = method)
levels = [95]
```

Let's run the backtesting using the `cross_validation` method:
```{python}
#| label: run_the_backtesting
bkt_df = mlf.cross_validation(
        df = ts,
        step_size= step_size,
        n_windows=partitions,
        prediction_intervals=PredictionIntervals(n_windows=2, h=h),
        level= levels,
        h=h,
        fitted=True,)
```



```{python}
#| label: View_the_backtesting
bkt_df.head()
```


Last but not least, let's plot the results:
```{python}
#| label: Plot_the_results
from plotly.subplots import make_subplots
import plotly.graph_objects as go

partitions_labels =  bkt_df["cutoff"].unique()

ts_sub = ts[ts["ds"] > ts["ds"].max() -  datetime.timedelta(hours = 24 * 7)]
fig = make_subplots(rows=partitions, cols=1, subplot_titles= ["Partitions: " + str(i) for i in partitions_labels])


r = 1

for i in partitions_labels:
    if r == 1:
        showlegend = True
    else:
        showlegend = False
    bkt_sub = bkt_df[bkt_df["cutoff"] == i]
    fig.append_trace(go.Scatter(x= ts_sub["ds"], y=ts_sub["y"], legendgroup = "actual", showlegend = showlegend, mode='lines', name='Actual', line=dict(color='#023047', width=2)), row = r, col = 1)
    fig.append_trace(go.Scatter(x=bkt_sub["ds"], y= bkt_sub["knn"], mode='lines', name='k-nearest neighbors', legendgroup = "knn", showlegend = showlegend, line=dict(color='#2a9d8f', width=1.5, dash = "dash")), row = r, col = 1)
    fig.append_trace(go.Scatter(x=bkt_sub["ds"], y= bkt_sub["mlp"], mode='lines', name='Multi-layer Perceptron',legendgroup = "mlp", showlegend = showlegend, line=dict(color='#0077b6', width=1.5, dash = "dot")), row = r, col = 1)
    fig.append_trace(go.Scatter(x=bkt_sub["ds"], y= bkt_sub["enet"], mode='lines', name='ElasticNet',legendgroup = "enet", showlegend = showlegend, line=dict(color='#ffc8dd', width=1.5, dash = "dot")), row = r, col = 1)
    r = r + 1


fig.update_layout(height=600)
fig.show()

```



## Scoring the models

Let's reformat the data, transform the backtesting table - `bkt_df` from wide to long.

We will use the `melt` function transition the table into long format, where we assign the transform fields names into new column named `model_label` and the corresponding values into the `value` column:
```{python}
#| label: Melt_the_table
models = list(ml_models.keys()) 
bkt_long = pd.melt(
    bkt_df,
    id_vars=["unique_id", "ds", "cutoff", "y"],
    value_vars=models + [f"{model}-lo-95" for model in models] \
    				  + [f"{model}-hi-95" for model in models],
    var_name="model_label",
    value_name="value",
)


bkt_long.head()
```

We will use the following function to relabel the forecast and prediction intervals values into `forecast`, `lower` and `upper`:
```{python}
#| label: Relabel the PI field
def split_model_confidence(model_name):
    if "-lo-95" in model_name:
        return model_name.replace("-lo-95", ""), "lower"
    elif "-hi-95" in model_name:
        return model_name.replace("-hi-95", ""), "upper"
    else:
        return model_name, "forecast"

bkt_long["model_label"],\
bkt_long["type"] = zip(*bkt_long["model_label"].map(split_model_confidence))
bkt_long.head()
```

Next, we will relabel the partitions values into a numeric value represents their order. First, let's create mapping between the `cutoff` field and the partition number:

```{python}
#| label: Partition mapping
cutoff = bkt_long["cutoff"].unique()
partitions_mapping = pd.DataFrame({"cutoff": cutoff, 
	"partition": range(1, len(cutoff) + 1)})

partitions_mapping.head()
```

In addition, we will map the model functions name to the labels we created and merge it later with the backtesting table:

```{python}
model_label = list(ml_models.keys()) 
model_name = [type(s).__name__ for s in list(ml_models.values())] 

models_mapping = pd.DataFrame({"model_label": model_label, "model_name": model_name})

models_mapping
```

Next, let's use the `pivot` function to pivot the `type` filed into three new fields and merge it with the partitions mapping table:
```{python}
#| label: Pivot
bkt = (bkt_long
.pivot(index = ["unique_id", "ds", "model_label","cutoff", "y"], columns = "type", values = "value")
.reset_index()
.merge(partitions_mapping, how = "left", on = ["cutoff"])
.merge(models_mapping, how = "left", on = ["model_label"])
)
bkt.head()
```

Now we can score the models results using the following helpers functions:

```{python}
#| label: Scoring helpers functions
from statistics import mean
def mape(y, yhat):
    mape = mean(abs(y - yhat)/ y) 
    return mape

def rmse(y, yhat):
    rmse = (mean((y - yhat) ** 2 )) ** 0.5
    return rmse

def coverage(y, lower, upper):
    coverage = sum((y <= upper) & (y >= lower)) / len(y)
    return coverage
```


```{python}
#| label: model_score_function
def score(df):
    mape_score = mape(y = df["y"], yhat = df["forecast"])
    rmse_score = rmse(y = df["y"], yhat = df["forecast"])
    coverage_score = coverage(y = df["y"], lower = df["lower"], upper = df["upper"])
    cols = ["mape","rmse", "coverage"]
    d = pd.Series([mape_score, rmse_score,  coverage_score], index=cols)

    return d
```


We will group by backtesting table by the series unique id, model label and partition and calculate its score:

```{python}
#| label: Score_the_backtesting
score_df = (bkt
.groupby(["unique_id", "model_label", "model_name", "partition"])[["unique_id", "model_label", "model_name", "partition", "y", "forecast", "lower", "upper"]]
.apply(score)
.reset_index()
)

score_df
```

## Logging the Results to MLflow

Let's load the MLflow library and define the experiment name:
```{python}
import mlflow
import datetime
experiment_name = "ml_models"
mlflow_path = "file:///mlruns"
```

We will log the backtesting parameters at tag:

```{python}
tags = {"h": h,
"step_size": step_size,
"partitions": partitions,
"intervals_type": "ConformalIntervals",
"intervals_h": h,
"intervals_n_windows": n_windows,
"intervals_method": "conformal_distribution",
"levels": levels }
```


```{python}
try:
    mlflow.create_experiment(name = experiment_name,
                            artifact_location= mlflow_path,
                            tags = tags)
    meta = mlflow.get_experiment_by_name(experiment_name)
    print(f"Set a new experiment {experiment_name}")
    print("Pulling the metadata") 
except: 
    print(f"Experiment {experiment_name} exists, pulling the metadata")
    meta = mlflow.get_experiment_by_name(experiment_name)
```


```{python}
run_time = datetime.datetime.now().strftime("%Y-%m-%d %H-%M-%S")
```


```{python}
for index, row in score_df.iterrows():
    run_name = row["model_label"] + "-" + run_time 
    with mlflow.start_run(experiment_id = meta.experiment_id, run_name = run_name,
                tags = {"type": "backtesting","partition": row["partition"], 
                "unique_id": row["unique_id"],"model_label": row["model_label"],
                "model_name": row["model_name"],"run_name": run_name}) as run:
        model_params = ml_models[row["model_label"]].get_params() 
        model_params["model_name"] =  row["model_name"]
        model_params["model_label"] =  row["model_label"]
        model_params["partition"] =  row["partition"]
        model_params["lags"] =  list(range(1, 24))
        model_params["date_features"] = ["month", "day", "dayofweek","week", "hour"]
        mlflow.log_params(model_params)
        mlflow.log_metric("mape", row["mape"])
        mlflow.log_metric("rmse", row["rmse"])
        mlflow.log_metric("coverage", row["coverage"])
```



