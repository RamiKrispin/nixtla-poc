---
title: "Training Models with Backtesting"
format:
  html:
    code-fold: false
jupyter: python3
---

## Load Libraries


```{python}
#| label: Loading libraries
import pandas as pd
import numpy as np
import requests
import json
import os
import mlflow
import datetime
import plotly.graph_objects as go
import mlflow

from mlforecast import MLForecast
from mlforecast.target_transforms import Differences
from mlforecast.utils import PredictionIntervals
from window_ops.expanding import expanding_mean
from lightgbm import LGBMRegressor
from xgboost import XGBRegressor
from sklearn.linear_model import Lasso, LinearRegression, Ridge
from sklearn.neural_network import MLPRegressor
from sklearn.ensemble import RandomForestRegressor
from utilsforecast.plotting import plot_series
from statistics import mean
import plotly.express as px
```


Load metadata
```{python}
#| label: Load the metadata
raw_json = open("./settings/settings.json")
meta_json = json.load(raw_json)
backtesting_path = meta_json["data"]["backtesting_path"]
```

## Load the Data

Reformat the data

```{python}
#| label: Load the data

ts = pd.read_csv("data/data.csv")
ts["ds"] = pd.to_datetime(ts["ds"])
ts = ts.sort_values("ds")
ts.head()
```

Subset for the last 25 months:
```{python}
#| label: Subset the data
end = ts["ds"].max()
start = end - datetime.timedelta(hours = 24 * 31 * 25)
ts = ts[ts["ds"] >= start]
```

## Set the Backtesting

Define the forecasting models:




```{python}
#| label: Set the model obj


ml_models = {
    "lightGBM": LGBMRegressor(verbosity=-1),
    "xgboost": XGBRegressor(),
    "linear_regression": LinearRegression(),
    "lasso": Lasso(),
    "ridge": Ridge()
}

# ml_models = [lgb, xgb, lm, lasso, ridge]

mlf = MLForecast(
    models= ml_models,  
    freq='h', 
    lags=list(range(1, 24)),  
    date_features=["month", "day", "dayofweek", "week", "hour"]
    )
```

```{python}
#| label: Backtesting settings
h = 72
step_size = 24
partitions = 10
n_windows = 5
method = "conformal_distribution"
pi = PredictionIntervals(h=h, n_windows = n_windows , method = method)
levels = [95]
```



```{python}
#| label: Run the backtesting
bkt_df = mlf.cross_validation(
        df = ts,
        h = h,
        step_size = step_size,
        n_windows = partitions,
        prediction_intervals = pi, 
        level = levels)

```



```{python}
#| label: Review the bkt 
bkt_df.head()
```



```{python}
#| label: Review a model 
bkt_df[["ds", "lightGBM", "lightGBM-lo-95",	"lightGBM-hi-95"]].head()
```



```{python}
models = list(ml_models.keys()) 
bkt_long = pd.melt(
    bkt_df,
    id_vars=["unique_id", "ds", "cutoff", "y"],
    value_vars=models + [f"{model}-lo-95" for model in models] \
    				  + [f"{model}-hi-95" for model in models],
    var_name="model_label",
    value_name="value",
)

bkt_long.head()
```



```{python}
def split_model_confidence(model_name):
    if "-lo-95" in model_name:
        return model_name.replace("-lo-95", ""), "lower"
    elif "-hi-95" in model_name:
        return model_name.replace("-hi-95", ""), "upper"
    else:
        return model_name, "forecast"

bkt_long["model_label"],\
bkt_long["type"] = zip(*bkt_long["model_label"].map(split_model_confidence))
```


```{python}
cutoff = bkt_long["cutoff"].unique()
partitions_mapping = pd.DataFrame({"cutoff": cutoff, 
	"partition": range(1, len(cutoff) + 1)})

partitions_mapping.head()
bkt_long = bkt_long.merge(partitions_mapping, how = "left", on = ["cutoff"])

bkt_long.head()
```

```{python}
#| label: Reformat the bkt obj
model_label = list(ml_models.keys()) 
model_name = [type(s).__name__ for s in list(ml_models.values())]  
lower = [s + "-lo-95" for s in model_label]
upper= [s + "-hi-95" for s in model_label]
models_mapping = pd.DataFrame({"model_label": model_label, 
            "model_name": model_name})


d1 = pd.melt(bkt_df, id_vars= ["unique_id", "ds", "cutoff"], 
value_vars= model_label, var_name = "model_label" , value_name = "forecast")
d2 = pd.melt(bkt_df, id_vars= ["unique_id", "ds", "cutoff"], 
value_vars= lower, var_name = "model_label" , value_name = "lower")
d2["model_label"] = d2["model_label"].str.replace("-lo-95", "")
d3 = pd.melt(bkt_df, id_vars= ["unique_id", "ds", "cutoff"], 
value_vars= upper, var_name = "model_label", value_name = "upper")
d3["model_label"] = d3["model_label"].str.replace("-hi-95", "")

bkt_long = (
    d1
.merge(right = d2, how = "left", on = ["unique_id", "ds", "cutoff", "model_label"])
.merge(right = d3, how = "left", on = ["unique_id", "ds", "cutoff", "model_label"])
.merge(right =  models_mapping, how = "left", on = ["model_label"])
.merge(right =  ts, how = "left", on = ["unique_id", "ds"])
)
bkt_long.head()
```



```{python}
#| label: Add partitions
cutoff = bkt_long["cutoff"].unique()
partitions_mapping = pd.DataFrame({"cutoff": cutoff, "partition": range(1, len(cutoff) + 1)})

partitions_mapping

bkt_long = bkt_long.merge(partitions_mapping, how = "left", on = ["cutoff"])
bkt_long.head()
```


```{python}
#| label: Score the models
def mape(y, yhat):
    mape = mean(abs(y - yhat)/ y) 
    return mape

def rmse(y, yhat):
    rmse = (mean((y - yhat) ** 2 )) ** 0.5
    return rmse

def coverage(y, lower, upper):
    coverage = sum((y <= upper) & (y >= lower)) / len(y)
    return coverage


def score(df):
    mape_score = mape(y = df["y"], yhat = df["forecast"])
    rmse_score = rmse(y = df["y"], yhat = df["forecast"])
    coverage_score = coverage(y = df["y"], lower = df["lower"], upper = df["upper"])
    cols = ["mape","rmse", "coverage"]
    d = pd.Series([mape_score, rmse_score,  coverage_score], index=cols)

    return d

score_df = (bkt_long
.groupby(["unique_id", "model_label", "model_name", "partition"])
.apply(score)
.reset_index()
)

score_df.head()
```








```{python}
#| label: Calculate the leaderboard
leaderboard = score_df.groupby(["unique_id", "model_label", "model_name"]).agg({"mape": "mean", "rmse": "mean", "coverage": "mean"}).reset_index()

leaderboard.sort_values(by = ["mape"])
```




```{python}
#| label: Plot the error rate
fig = px.box(score_df, x="model_label", y="rmse", color="model_label")
fig.update_traces(boxpoints = 'all', jitter = 0.3, pointpos = -1.8, showlegend = False)

fig.update_layout(
    title="Error Distribution",
    xaxis_title="Model",
    yaxis_title="RMSE",
    font=dict(family="Arial", size=14, color="black")
)

fig.show()
```


## Logging the Results with MLflow

```{python}
#| label: Setting MLflow exp
import mlflow

experiment_name = "ml_forecast"

mlflow_path = "file:///mlruns"

tags = {"h": h,
"step_size": step_size,
"partitions": partitions,
"intervals_type": "ConformalIntervals",
"intervals_h": h,
"intervals_n_windows": n_windows,
"intervals_method": "conformal_distribution",
"levels": levels }



try:
    mlflow.create_experiment(name = experiment_name,
                            artifact_location= mlflow_path,
                            tags = tags)
    meta = mlflow.get_experiment_by_name(experiment_name)
    print(f"Set a new experiment {experiment_name}")
    print("Pulling the metadata")
except:
    print(f"Experiment {experiment_name} exists, pulling the metadata")
    meta = mlflow.get_experiment_by_name(experiment_name)
```

```{python}
#| label: Log the results
run_time = datetime.datetime.now().strftime("%Y-%m-%d %H-%M-%S")
for index, row in score_df.iterrows():
    run_name = row["model_label"] + "-" + run_time 
    with mlflow.start_run(experiment_id = meta.experiment_id, 
                run_name = run_name,
                tags = {"type": "backtesting",
                "partition": row["partition"], 
                "unique_id": row["unique_id"],
                "model_label": row["model_label"],
                "model_name": row["model_name"],
                "run_name": run_name}) as run:

        model_params = ml_models[row["model_label"]].get_params() 
        model_params["model_name"] =  row["model_name"]
        model_params["model_label"] =  row["model_label"]
        model_params["partition"] =  row["partition"]
        model_params["lags"] =  list(range(1, 24))
        model_params["date_features"] = ["month", "day", "dayofweek","week", "hour"]
        mlflow.log_params(model_params)
        mlflow.log_metric("mape", row["mape"])
        mlflow.log_metric("rmse", row["rmse"])
        mlflow.log_metric("coverage", row["coverage"])
```



 ```{python}
 #| label: Save the data
 bkt_df.to_csv(backtesting_path, index = False)
 ```



## Additional tuning

 ```{python}
 ml_models2 = {
    "lightGBM1": LGBMRegressor(n_estimators = 100, learning_rate= 0.1),
    "lightGBM2": LGBMRegressor(n_estimators = 250, learning_rate= 0.1),
    "lightGBM3": LGBMRegressor(n_estimators = 500, learning_rate= 0.1),
    "lightGBM4": LGBMRegressor(n_estimators = 100, learning_rate= 0.05),
    "lightGBM5": LGBMRegressor(n_estimators = 250, learning_rate= 0.05),
    "lightGBM6": LGBMRegressor(n_estimators = 500, learning_rate= 0.05),
}

# ml_models = [lgb, xgb, lm, lasso, ridge]

mlf2 = MLForecast(
    models= ml_models2,  
    freq='h', 
    lags=list(range(1, 24)),  
    date_features=["month", "day", "dayofweek", "week", "hour"]
    )
```

 ```{python}
bkt_df2 = mlf2.cross_validation(
        df = ts,
        h = h,
        step_size = step_size,
        n_windows = partitions,
        prediction_intervals = pi, 
        level = levels)

```



```{python}
model_label = list(ml_models2.keys()) 
model_name = [type(s).__name__ for s in list(ml_models2.values())]  
lower = [s + "-lo-95" for s in model_label]
upper= [s + "-hi-95" for s in model_label]
models_mapping = pd.DataFrame({"model_label": model_label, 
            "model_name": model_name})


d1 = pd.melt(bkt_df2, id_vars= ["unique_id", "ds", "cutoff"], 
value_vars= model_label, var_name = "model_label" , value_name = "forecast")
d2 = pd.melt(bkt_df2, id_vars= ["unique_id", "ds", "cutoff"], 
value_vars= lower, var_name = "model_label" , value_name = "lower")
d2["model_label"] = d2["model_label"].str.replace("-lo-95", "")
d3 = pd.melt(bkt_df2, id_vars= ["unique_id", "ds", "cutoff"], 
value_vars= upper, var_name = "model_label", value_name = "upper")
d3["model_label"] = d3["model_label"].str.replace("-hi-95", "")

bkt_long2 = (
    d1
.merge(right = d2, how = "left", on = ["unique_id", "ds", "cutoff", "model_label"])
.merge(right = d3, how = "left", on = ["unique_id", "ds", "cutoff", "model_label"])
.merge(right =  models_mapping, how = "left", on = ["model_label"])
.merge(right =  ts, how = "left", on = ["unique_id", "ds"])
)
bkt_long2.head()

bkt_long2 = bkt_long2.merge(partitions_mapping, how = "left", on = ["cutoff"])

score_df2 = (bkt_long2
.groupby(["unique_id", "model_label", "model_name", "partition"])
.apply(score)
.reset_index()
)

score_df2.head()
```




```{python}
leaderboard2 = score_df2.groupby(["unique_id", "model_label", "model_name"]).agg({"mape": "mean", "rmse": "mean", "coverage": "mean"}).reset_index()

leaderboard2.sort_values(by = ["mape"])
```



```{python}
run_time = datetime.datetime.now().strftime("%Y-%m-%d %H-%M-%S")
for index, row in score_df2.iterrows():
    run_name = row["model_label"] + "-" + run_time 
    with mlflow.start_run(experiment_id = meta.experiment_id, 
                run_name = run_name,
                tags = {"type": "backtesting",
                "partition": row["partition"], 
                "unique_id": row["unique_id"],
                "model_label": row["model_label"],
                "model_name": row["model_name"],
                "run_name": run_name}) as run:

        model_params = ml_models2[row["model_label"]].get_params() 
        model_params["model_name"] =  row["model_name"]
        model_params["model_label"] =  row["model_label"]
        model_params["partition"] =  row["partition"]
        model_params["lags"] =  list(range(1, 24))
        model_params["date_features"] = ["month", "day", "dayofweek","week", "hour"]
        mlflow.log_params(model_params)
        mlflow.log_metric("mape", row["mape"])
        mlflow.log_metric("rmse", row["rmse"])
        mlflow.log_metric("coverage", row["coverage"])
```

TODO
merge the backtesting tables and save to csv fie
 ```{python}
 #| label: Save the data
 bkt_df2.to_csv(backtesting_path, index = False)
 ```

